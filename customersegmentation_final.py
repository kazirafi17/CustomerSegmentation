# -*- coding: utf-8 -*-
"""CustomerSegmentation_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jf_3nrFhN9Moly9Wt6ULQA1xXessZTz_

<a href="https://colab.research.google.com/github/kazirafi17/CustomerSegmentation/blob/main/CustomerSegmentation.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# Project Name : Customer Segmentation

#### Description:

`Customer segmentation` simply means grouping your customers according to various characteristics (for example grouping customers by location).It’s a way for organizations to understand their customers. Knowing the differences between customer groups, it’s easier to make strategic decisions regarding product growth and marketing.

But, doing segmentation manually can be exhausting. Why not employ machine learning to do it for us? In this machine learning project, I’ll show you how to do just that. Stay tunned!

# 1. Import Libraries and framework
"""

# import required libraries for dataframe and visualization

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
import datetime as dt
import plotly.express as px

# import required libraries for clustering
import sklearn
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import linkage
from scipy.cluster.hierarchy import dendrogram
from scipy.cluster.hierarchy import cut_tree
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans

import warnings
warnings.filterwarnings('ignore')

"""# 2. Load/Import and Read the datasets"""

df = pd.read_excel('/content/drive/MyDrive/Online Retail.xlsx')

df.head(5) # Display the first 5 rows of the DataFrame

"""# 3. Data Cleaning and Preprocessing

#### 3.1 Explore the datasets
"""

df.info() # Display information about the DataFrame, including the data types and non-null values

df.describe()  # Display summary statistics for numerical columns

# Calculating the Missing Values % contribution in DF

df_null = round(100*(df.isnull().sum())/len(df), 2)
df_null

df.nunique()  # Display the number of unique values in each column

df.duplicated().sum() # Check for duplicate rows in the DataFrame and return a boolean Series

"""#### Import insights:

*   0.27% and 25.16% of `Description` and `CustomerID` values are missing respectively.
*   Total 8 columns including 4 object, 2 floats, 1 int and 1 datetime. Dataset contains 5.4Lakhs rows.
*   It contains 5268 duplicated values.

#### 3.2 Handling Missing Values and Duplicates
"""

df.drop_duplicates(inplace=True) # Remove duplicate rows from the DataFrame

df['Description'].mode() # Find the mode of Description feature.
df['Description'].fillna(df['Description'].mode()[0], inplace=True) # Fill the missing values of Description column by it's mode value.

df = df.dropna() # Droping rows having missing values

"""# 4. Feature Engineering"""

# Create a new feature

df['Amount'] = df['Quantity'] * df['UnitPrice']

# Calculate sum of amount spend by each customers

df_amnt = df.groupby('CustomerID')['Amount'].sum().reset_index()
df_amnt.head()

# Filter the DataFrame to include only the customers who have an 'Amount' less than 0
# This creates a new DataFrame containing customers who never bought anything or gave back products
df_non_buyers = df_amnt[df_amnt['Amount'] <= 0]

# Print the number of customers who didn't buy anything or gave back products
print(df_non_buyers.shape[0], "Number of customers didn't buy anything or gave back products. Samples are:")

# Filter the original DataFrame df to include only those customers whose CustomerID
# is present in the df_non_buyers DataFrame
df_filtered = df[df['CustomerID'].isin(df_non_buyers['CustomerID'])]
df_filtered.sample(3)
#df_filtered.shape

"""**Negative values** in the graph typically indicate returns or cancellations of products. When customers return items, the amount is recorded as negative to reflect the subtraction from the total sales and inventory.

**Returns**: Customers returned items, which results in a negative quantity to account for the items being added back to inventory.

**Cancellations**: Orders that were canceled and the items were not actually sold, thus the negative quantity.
"""

df = df.drop(df_filtered.index)

# Group the DataFrame 'df' by 'CustomerID' and count the unique 'InvoiceNo' values for each customer.
# This creates a DataFrame with 'CustomerID' and 'InvoiceNo' columns,
# where 'InvoiceNo' represents the count of unique invoices for each customer.
df_invc = df.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()

# Rename the 'InvoiceNo' column to 'Frequency' to indicate the number of unique invoices for each customer.
df_invc.rename(columns={'InvoiceNo': 'Frequency'}, inplace=True)

# Display the first few rows of the resulting DataFrame to verify the results.
df_invc.head()

# Convert to datetime to proper datatype
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%d-%m-%Y %H:%M')

# Compute the maximum date to know the last transaction date in our dataset
max_date = max(df['InvoiceDate'])

# Calculate the time difference between the maximum date in the dataset and the transaction date for each record
df['DaysSinceLastPurchase'] = max_date - df['InvoiceDate']

# Extract the days only from the time difference
df['DaysSinceLastPurchase'] = df['DaysSinceLastPurchase'].dt.days

# Group by CustomerID and calculate the minimum difference (Recency) for each customer
df_date = df.groupby('CustomerID')['DaysSinceLastPurchase'].min().reset_index()

# Display the first 5 rows of the resulting DataFrame
df_date.head()

# Merge df_amnt and df_invc on 'CustomerID'
df_merged1 = pd.merge(df_amnt, df_invc, on='CustomerID', how='inner')

# Merge the result with df_date on 'CustomerID'
df_final = pd.merge(df_merged1, df_date, on='CustomerID', how='inner')
df_final.head(5)

# Calculate the total spending per visit for each customer by dividing the amount by frequency.
# The new column is named 'Avg_Spending_Per_Visit' to indicate average spending per visit.
# Round the result to two decimal places.
df_final['Avg_Spending_Per_Visit'] = (df_final['Amount'] / df_final['Frequency']).round(2)

# Display the first few rows of the DataFrame to verify the results.
df_final.head()

"""# 4. Explanatory Data Analysis

#### 4.1 Uni-variate Analysis for continous variables
"""

# Sort the dataframe by 'Frequency' in descending order and select the top 10
top_10_customers = df_final.nlargest(10, 'Frequency')

# Sort the dataframe by 'Frequency' in ascending order and select the bottom 10
least_10_customers = df_final.nsmallest(10, 'Frequency')

# Combine the top 10 and bottom 10 customers
combined = pd.concat([top_10_customers, least_10_customers])

# Plotting the bar chart for combined customers with CustomerID on x-axis
plt.bar(combined['CustomerID'].astype(str), combined['Frequency'], color='purple', edgecolor='black')
plt.gcf().set_size_inches(10, 8)
plt.title('Top 10 and Least 10 Frequent Customers')
plt.xlabel('CustomerID')
plt.ylabel('Frequency')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Plotting univariate analysis in 4 subplots
fig, axs = plt.subplots(1, 4, figsize=(25,8))

# Histogram of Amount
axs[0].hist(df_final['Amount'], bins=10, color='skyblue', edgecolor='black')
axs[0].set_title('Histogram of Amount')
axs[0].set_xlabel('Amount')
axs[0].set_ylabel('Counts')

# Histogram of Frequency
axs[1].hist(df_final['Frequency'], bins=10, color='navy', edgecolor='black')
axs[1].set_title('Histogram of Frequency')
axs[1].set_xlabel('Frequency')
axs[1].set_ylabel('Counts')

# Histogram of DaysSinceLastPurchase
axs[2].hist(df_final['DaysSinceLastPurchase'], bins=10, color='lightgreen', edgecolor='black')
axs[2].set_title('Histogram of DaysSinceLastPurchase')
axs[2].set_xlabel('Days Since Last Purchase')
axs[2].set_ylabel('Counts')

# Histogram of Avg_Spending_Per_Visit
axs[3].hist(df_final['Avg_Spending_Per_Visit'], bins=10, color='lightcoral', edgecolor='black')
axs[3].set_title('Histogram of Avg Spending Per Visit')
axs[3].set_xlabel('Avg Spending Per Visit')
axs[3].set_ylabel('Counts')

plt.tight_layout()
plt.show()

"""**Observations:**



*   Top 10 frequent customers visit 30 or more times and some customers visits for only one time.

*   Histogram of Amount: A significant number of purchases are under 2000 units.There are very few purchases with high amounts, with the frequency dropping sharply as the amount increases.

*   Histogram of Frequency:The frequency of purchases per customer is heavily skewed towards the lower end.Most customers have a low purchase frequency, typically between 1 and 5.A small number of customers have higher purchase frequencies, but these are much less common.

*  Histogram of DaysSinceLastPurchase: This distribution is right-skewed, indicating that many customers made recent purchases.A large number of customers made their last purchase very recently (0-50 days).There is a gradual decline in the number of customers as the days since the last purchase increase.



*   Histogram of Avg Spending Per Visit:The average spending per visit is concentrated around lower values, with a significant number of visits having spending around 0 to 500 units.There is a steep drop-off in frequency as the average spending per visit increases.Very few visits have an average spending that is very high (above 1000 units).

4.2 Multi-variate Analysis for continous variables
"""

fig, axs = plt.subplots(1, 4, figsize=(25, 6))

# Create a sub scatter plot of DaysSinceLastPurchase vs Amount
axs[0].scatter(df_final['DaysSinceLastPurchase'], df_final['Amount'], color='purple')
axs[0].set_title('Scatter Plot of DaysSinceLastPurchase vs Amount')
axs[0].set_xlabel('Days Since Last Purchase')
axs[0].set_ylabel('Amount')

# Create a sub scatter plot of DaysSinceLastPurchase vs Avg_Spending_Per_Visit
axs[1].scatter(df_final['DaysSinceLastPurchase'], df_final['Avg_Spending_Per_Visit'], color='navy')
axs[1].set_title('Scatter Plot of DaysSinceLastPurchase vs Avg_Spending_Per_Visit')
axs[1].set_xlabel('Days Since Last Purchase')
axs[1].set_ylabel('Avg Spending Per Visit')

# Create a sub scatter plot of DaysSinceLastPurchase vs Frequency
axs[2].scatter(df_final['DaysSinceLastPurchase'], df_final['Frequency'], color='skyblue')
axs[2].set_title('Scatter Plot of DaysSinceLastPurchase vs Frequency')
axs[2].set_xlabel('Days Since Last Purchase')
axs[2].set_ylabel('Frequency')

# Create a sub scatter plot of Frequency vs Amount
axs[3].scatter(df_final['Frequency'], df_final['Amount'], color='lightcoral')
axs[3].set_title('Scatter Plot of Frequency vs Amount')
axs[3].set_xlabel('Frequency')
axs[3].set_ylabel('Amount')

# Add grid lines to all subplots
for ax in axs:
    ax.grid(True)

plt.show()

"""**General Insights:**

*   Recent and frequent customers tend to spend more, both in total and per visit.
*   Customers who have not made purchases for a longer period tend to have lower overall spending and frequency.

**`Encouraging frequent visits and reducing the time between purchases could potentially increase total spending.`**

#### Removing Outliers
"""

# Removing (statistical) outliers for DaysSinceLastPurchase
Q1 = df_final['DaysSinceLastPurchase'].quantile(0.05)
Q3 = df_final['DaysSinceLastPurchase'].quantile(0.80)
IQR = Q3 - Q1
df_final = df_final[(df_final['DaysSinceLastPurchase'] >= Q1 - 1.5 * IQR) & (df_final['DaysSinceLastPurchase'] <= Q3 + 1.5 * IQR)]

# Removing (statistical) outliers for Avg_Spending_Per_Visit
Q1 = df_final['Avg_Spending_Per_Visit'].quantile(0.05)
Q3 = df_final['Avg_Spending_Per_Visit'].quantile(0.95)  # Corrected to 95th percentile
IQR = Q3 - Q1
df_final = df_final[(df_final['Avg_Spending_Per_Visit'] >= Q1 - 1.5 * IQR) & (df_final['Avg_Spending_Per_Visit'] <= Q3 + 1.5 * IQR)]

# Removing (statistical) outliers for Amount
Q1 = df_final['Amount'].quantile(0.05)
Q3 = df_final['Amount'].quantile(0.95)  # Corrected to 95th percentile
IQR = Q3 - Q1
df_final = df_final[(df_final['Amount'] >= Q1 - 1.5 * IQR) & (df_final['Amount'] <= Q3 + 1.5 * IQR)]

# Removing (statistical) outliers for Frequency
Q1 = df_final['Frequency'].quantile(0.05)
Q3 = df_final['Frequency'].quantile(0.95)  # Corrected to 95th percentile
IQR = Q3 - Q1
df_final = df_final[(df_final['Frequency'] >= Q1 - 1.5 * IQR) & (df_final['Frequency'] <= Q3 + 1.5 * IQR)]

"""#### Scaling:

Scaling data before plotting ensures that the variables are on the same scale, making it easier to interpret the box plot, especially when variables have different units or ranges. Adjust the parameters and customization as per your specific data and visualization requirements.
"""

# Define attributes to plot
attributes = ['DaysSinceLastPurchase', 'Avg_Spending_Per_Visit', 'Amount', 'Frequency']

# Perform standardization
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_final[attributes])

# Convert scaled data back to DataFrame
df_scaled = pd.DataFrame(scaled_data, columns=attributes)

# Create the box plot using Plotly Express
fig = px.box(df_scaled.melt(var_name='Attributes', value_name='Range'),
             y='Range', x='Attributes', points='outliers', boxmode='group',
             title="Outliers Variable Distribution (Scaled)",
             labels={'Attributes': 'Attributes', 'Range': 'Range'},
             color_discrete_sequence=['navy'],  # Ensure all elements are purple
             width=800, height=600)

# Update layout with axis titles
fig.update_layout(
    xaxis=dict(title="Attributes", title_font=dict(size=14)),
    yaxis=dict(title="Range (Standardized)", title_font=dict(size=14)),
    showlegend=False
)

# Show the plot
fig.show()

final_data = df_scaled.drop(['Amount','Frequency'],axis=1)

"""# Building the model:"""

# Define range of k values to test
k_values = range(1, 11)
wcss = []  # Within cluster sum of squares

# Calculate WCSS for each k
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(final_data)
    wcss_score = kmeans.inertia_
    wcss.append(wcss_score)
    print(f"The elbow score for k={k}: is {wcss_score}")

# Plotting the elbow curve
plt.figure(figsize=(8, 5))
plt.plot(k_values, wcss, marker='o', linestyle='-', color='purple', linewidth=2)
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
plt.title('Elbow Method for Optimal k')
plt.xticks(k_values)
plt.grid(True)
plt.tight_layout()
plt.show()

"""The decrease after k=3 is more gradual, suggesting that the optimal number of clusters is likely around **k=2 or k=3.**


"""

# Silhouette analysis
range_n_clusters = [2, 3, 4, 5, 6, 7, 8]

for num_clusters in range_n_clusters:

    # intialise kmeans
    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)
    kmeans.fit(final_data)

    cluster_labels = kmeans.labels_

    # silhouette score
    silhouette_avg = silhouette_score(final_data, cluster_labels)
    print("For n_clusters={0}, the silhouette score is {1}".format(num_clusters, silhouette_avg))

"""*  The silhouette score is highest for **n_clusters=3** (0.551), indicating that three clusters provide the best-defined clusters among the tested values.

**Conclusion**


Based on both the elbow method and the silhouette scores:

The elbow method suggests that the optimal number of clusters might be around 3 or 4.
The silhouette score analysis indicates that 3 clusters provide the best separation.
Therefore, the most appropriate number of clusters for your data seems to be 3.
"""

# Final model with k=3
kmeans = KMeans(n_clusters=3, max_iter=50)
kmeans.fit(final_data)

kmeans.labels_

# assign the label
df_final['Cluster_Id'] = kmeans.labels_
df_final

fig = px.box(df_final, x='Cluster_Id', y='DaysSinceLastPurchase',
             title="Cluster Id vs DaysSinceLastPurchase Box Plot",
             labels={'Cluster_Id': 'Cluster ID', 'DaysSinceLastPurchase': 'DaysSinceLastPurchase'},
             color='Cluster_Id')

fig.update_layout(
    xaxis=dict(title="Cluster ID", title_font=dict(size=14)),
    yaxis=dict(title="DaysSinceLastPurchase", title_font=dict(size=14)),
    showlegend=False,
    width=800,
    height=600
)

fig.show()

fig = px.scatter(df_final, x='Cluster_Id', y='Avg_Spending_Per_Visit', color='Frequency',
                 title='Cluster ID vs Avg_Spending_Per_Visit (Color Encoded by Frequency)',
                 labels={'Cluster_Id': 'Cluster ID', 'Avg_Spending_Per_Visit': 'Avg_Spending_Per_Visit', 'Frequency': 'Frequency'})

fig.update_layout(
    xaxis=dict(title="Cluster ID", title_font=dict(size=8)),
    yaxis=dict(title="Avg_Spending_Per_Visitunt", title_font=dict(size=14)),
    showlegend=True,
    width=800,
    height=600
)

fig.show()

# Create a custom color palette with Red, Green, and Blue
custom_palette = sns.color_palette(["#FF0000", "#00FF00", "#0000FF"])

# Create a scatter plot matrix with separate plots for each cluster, custom palette, and a larger size
sns.set(style="ticks")
sns.pairplot(df_final, hue='Cluster_Id', vars=['Avg_Spending_Per_Visit', 'DaysSinceLastPurchase'], palette=custom_palette, height=4, aspect=1.5)
plt.suptitle('Cluster ID vs Avg_Spending_Per_Visit & DaysSinceLastPurchase', y=1.02)
plt.show()

# Create a custom color palette with Red, Green, and Blue
custom_palette = sns.color_palette(["#FF0000", "#00FF00", "#0000FF"])

# Create a scatter plot matrix with separate plots for each cluster, custom palette, and a larger size
sns.set(style="ticks")
sns.pairplot(df_final, hue='Cluster_Id', vars=['Frequency', 'Amount'], palette=custom_palette, height=4, aspect=1.5)
plt.suptitle('Cluster ID vs Avg_Spending_Per_Visit & DaysSinceLastPurchase', y=1.02)
plt.show()

# Pairwise scatter plots for clustering (2D)
plt.figure(figsize=(15, 8))  # Adjust the figure size here


# Amount vs. Frequency
sns.scatterplot(x='Avg_Spending_Per_Visit', y='DaysSinceLastPurchase', hue='Cluster_Id', data=df_final, palette='Set1')
plt.title('Avg_Spending_Per_Visit vs. DaysSinceLastPurchase')
plt.show()

import plotly.express as px

df_final['Cluster_3D'] = kmeans.fit_predict(final_data)

# Create a 3D scatter plot matrix with Plotly
fig = px.scatter_3d(df_final, x='Avg_Spending_Per_Visit', y='Frequency', z='DaysSinceLastPurchase', color='Cluster_3D',
                     labels={'Avg_Spending_Per_Visit': 'Avg_Spending_Per_Visit', 'Frequency': 'Frequency', 'DaysSinceLastPurchase': 'DaysSinceLastPurchase', 'Cluster_3D': 'Cluster'})

fig.update_layout(
    scene=dict(
        xaxis_title='Avg_Spending_Per_Visit',
        yaxis_title='Frequency',
        zaxis_title='DaysSinceLastPurchase',
    ),
    title='Clustering by Avg_Spending_Per_Visit, Frequency, and DaysSinceLastPurchase',
    width=800,
    height=600
)

fig.show()

"""**Conclusion:**

It’s not wise to serve all customers with the same product model, email, text message campaign, or ad. Customers have different needs. A one-size-for-all approach to business will generally result in less engagement, lower-click through rates, and ultimately fewer sales. Customer segmentation is the cure for this problem.

Finding an optimal number of unique customer groups will help you understand how your customers differ, and help you give them exactly what they want. Customer segmentation improves customer experience and boosts company revenue. That’s why segmentation is a must if you want to surpass your competitors and get more customers. Doing it with machine learning is definitely the right way to go.

If you made it this far, thanks for your valuable time!
"""

import pickle

# Path to save the pickle file
pkl_filename = "customer_segmentationn.pkl"

# Save the model to a pickle file
with open(pkl_filename, 'wb') as file:
    pickle.dump(kmeans, file)

print(f"Model saved to {pkl_filename}")

